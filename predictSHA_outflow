#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun May  6 11:50:47 2018

@author: natnaelhamda
"""
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


import timeit

start = timeit.default_timer()

# Importing the training set

#dataset_tran_test_val_in = pd.read_csv('data_all_2000_2017_in.csv') #all inputs 2000 - 2017
dataset_tran_test_val_in = pd.read_csv('profile_model_in_out.csv') #all inputs 2000 - 2017

dataset_tran_test_out = pd.read_csv('data_all_2000_2016_out.csv') #all inputs 2000 - 2017


dataset_tran_test_val_in = np.array(dataset_tran_test_val_in)


dataset_tran_test_out = np.array(dataset_tran_test_out)


y_hat_shasta = dataset_tran_test_out[:,0] #We have training & testing data for Shasta outflow, no for validation

y_hat_keswick = dataset_tran_test_out[:,1]


# Feature Scaling
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler(feature_range = (0, 1)) #For inputs
sc1 = MinMaxScaler(feature_range = (0, 1)) #For Shashta
sc2 = MinMaxScaler(feature_range = (0, 1)) #For Keswick

data_all_2000_2018_in_scaled = sc.fit_transform(dataset_tran_test_val_in)

y_hat_shasta_scaled = sc1.fit_transform(y_hat_shasta.reshape(-1, 1)) #For the purpose of recalining the predict

y_hat_keswick_scaled = sc2.fit_transform(y_hat_keswick.reshape(-1, 1)) #For the purpose of recalining the predict

## Structureing the training and testing data

# Predictors (we should have a predictor for the validation data set as well)
training_set_scaled = data_all_2000_2018_in_scaled[:5273,[2, 15, 16, 17, 18, 26,27,28]]

testing_set_scaled = data_all_2000_2018_in_scaled[5273:6208,[2, 15, 16, 17, 18, 26,27,28]]

validation_set_scaled = data_all_2000_2018_in_scaled[6208:,[2,15, 16, 17, 18, 26,27,28]]

## Response (we should have responses only for training and testing data)
# Shasta
training_set_scaled_resp_shashta = y_hat_shasta_scaled[:5273,:]

testing_set_scaled_resp_shashta = y_hat_shasta_scaled[5273:6208,:]

## Keswick
#training_set_scaled_resp_keswick = y_hat_keswick_scaled[1:5274,:]
#
#testing_set_scaled_resp_kesiwck = y_hat_keswick_scaled[5274:6209,:]



# Creating a data structure with different lag timesteps and 1 output
lag_day = 2

X_train = []
y_train = []
for i in range(lag_day, len(training_set_scaled)):
    X_train.append(training_set_scaled[i-lag_day:i, :]) #Assuming 5 days lag time affects, here discharge temperature not included  
    y_train.append(training_set_scaled_resp_shashta[i]) #Shashta

X_train, y_train = np.array(X_train), np.array(y_train)

# Reshaping
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 8))

#test_set_scaled = testing_set_scaled_resp_shashta

X_test = []
y_test = []
for i in range(lag_day, len(testing_set_scaled_resp_shashta)):
    X_test.append(testing_set_scaled[i-lag_day:i, :])
    y_test.append(testing_set_scaled_resp_shashta[i])


X_test, y_test = np.array(X_test), np.array(y_test)

# Reshaping for keras formating
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 8))


# Building the RNN

# Importing the Keras libraries and packages
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

# Initialising the RNN
model = Sequential()

# Adding the first LSTM layer and some Dropout regularisation
#model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 26)))
model.add(LSTM(units = 8,  return_sequences = True, input_shape = (X_train.shape[1], 8)))
model.add(Dropout(0.2))

## Adding a second LSTM layer and some Dropout regularisation
#model.add(LSTM(units = 8,  return_sequences = True))
#model.add(Dropout(0.2))

## Adding a second LSTM layer and some Dropout regularisation
#model.add(LSTM(units = 16, return_sequences = True))
#model.add(Dropout(0.2))

# Adding a second LSTM layer and some Dropout regularisation
model.add(LSTM(units = 8))
model.add(Dropout(0.2))

# Adding the output layer
model.add(Dense(units = 1))


import keras

from keras import optimizers


import math
from keras.layers import Dense
from keras.optimizers import SGD

from keras.callbacks import LearningRateScheduler



def get_triangular_lr(epoch):
    """Given the inputs, calculates the lr that should be applicable for this iteration"""
    #num_iterations = 10000

    stepsize = 100
    base_lr = 0.0000001
    max_lr = 0.00001
    
    cycle = np.floor(1 + epoch/(2  * stepsize))
    x = np.abs(epoch/stepsize - 2 * cycle + 1)
    lrate = base_lr + (max_lr - base_lr) * np.maximum(0, (1-x))
    return lrate



epoch = 5000

# Compile model

ln_algorithm = optimizers.Adam(lr=0.000, beta_1=0.9, beta_2=0.999, epsilon=1e-08)

#ln_algorithm = optimizers.SGD(lr=0, momentum = 0.8, nesterov=True)

# learning schedule callback
lrate = LearningRateScheduler(get_triangular_lr)
callbacks_list = [lrate]

#compile the model
model.compile(optimizer = ln_algorithm, loss = 'mean_squared_error')

# Fit the model
history = model.fit(X_train, y_train, epochs = epoch, batch_size = 128, validation_data = (X_test, y_test),callbacks=callbacks_list,verbose=2)




#model.compile(optimizer = 'adam', loss = 'mean_squared_error')

#history = model.fit(X_train, y_train, epochs = 2000, batch_size = 32, validation_data = (X_test, y_test), verbose=2, shuffle=False)




## plot history
#
from matplotlib import pyplot
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()


############################################################################
#Making the predictions and visualising the results for the year 2017

# Getting the temperature data
#real_observation = dataset.iloc[4450:, 1].values

#real_observation_scaled = training_set_scaled_all[4450:, 1]

#test_set_scaled = training_set_scaled_all[4450:, 2:]


# Predictors (we should have a predictor for the validation data set as well)

######################################## Training ############################


X_valdidation = []

for i in range(lag_day, len(validation_set_scaled)):
    X_valdidation.append(validation_set_scaled[i-lag_day:i, :]) #predictors of the year 2017




X_valdidation = np.array(X_valdidation)

# Reshaping
X_valdidation = np.reshape(X_valdidation, (X_valdidation.shape[0], X_valdidation.shape[1], 8))


predicted_temp_scled = model.predict(X_valdidation)


predicted_temp = sc1.inverse_transform(predicted_temp_scled)


#All temperature data for simulation

data_2000_2017_temp = pd.read_csv('rawData_temp_2000_2017.csv', header = 0) 


#data_2000_2017_temp_grouped = data_2000_2017_temp.groupby('Date')


data_2000_2017_temp_values = data_2000_2017_temp.values

shsta_2000_2016_reshaped = data_2000_2017_temp_values[:,1].reshape((365, 17))


shsta_2000_2016_mean = np.mean(shsta_2000_2016_reshaped, axis=1)


shasta_2017 = pd.read_csv('2017_observed.csv', header = 0) 

shasta_2017 = shasta_2017.values

## Visualising the results: Raw
plt.plot(shasta_2017[lag_day:], color = 'red', label = 'Actual: 2017 ')
plt.plot(predicted_temp[1:], color = 'blue', label = 'Predicted: 2017')
plt.title('Shasta discharge Temprature')
plt.ylim(7, 12)
plt.grid(True)
plt.xlabel('Time, # days')
plt.ylabel('Temperature, in oC')
plt.legend()
plt.axis([0, 365, 6, 13])
plt.show()
#
from math import sqrt
from sklearn.metrics import mean_squared_error

#rmse_test = sqrt(mean_squared_error(shasta_2017[lag_day:], predicted_temp[1:]))
#print('Test RMSE: %.3f' % rmse_test)

rmse_validation = sqrt(mean_squared_error(shasta_2017[lag_day+1:], predicted_temp[:]))
print('Validation RMSE: %.3f' % rmse_validation)


stop = timeit.default_timer()

print(stop - start) 
